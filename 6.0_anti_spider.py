# 反爬

# 一、基于身份识别进行反爬
# 1 通过headers字段来反爬
# > headers中有很多字段，这些字段都有可能会被对方服务器拿过来进行判断是否为爬虫

# 1.1 通过headers中的User-Agent字段来反爬
# - 反爬原理：爬虫默认情况下没有User-Agent，而是使用模块默认设置
# - 解决方法：请求之前添加User-Agent即可；更好的方式是使用User-Agent池来解决（收集一堆User-Agent的方式，或者是随机生成User-Agent）

# 1.2 通过referer字段或者是其他字段来反爬
# - 反爬原理：爬虫默认情况下不会带上referer字段，服务器端通过判断请求发起的源头，以此判断请求是否合法
# - 解决方法：添加referer字段

# 1.3 通过cookie来反爬
# - 反爬原因：通过检查cookies来查看发起请求的用户是否具备相应权限，以此来进行反爬
# - 解决方案：进行模拟登陆，成功获取cookies之后在进行数据爬取

# 2 通过请求参数来反爬
# > 请求参数的获取方法有很多，向服务器发送请求，很多时候需要携带请求参数，通常服务器端可以通过检查请求参数是否正确来判断是否为爬虫

# 2.1 通过从html静态文件中获取请求数据(github登录数据)
# - 反爬原因：通过增加获取请求参数的难度进行反爬
# - 解决方案：仔细分析抓包得到的每一个包，搞清楚请求之间的联系

# 2.2 通过发送请求获取请求数据
# - 反爬原因：通过增加获取请求参数的难度进行反爬
# - 解决方案：仔细分析抓包得到的每一个包，搞清楚请求之间的联系，搞清楚请求参数的来源

# 2.3 通过js生成请求参数
# - 反爬原理：js生成了请求参数
# - 解决方法：分析js，观察加密的实现过程，通过js2py获取js的执行结果，或者使用selenium来实现

# 2.4 通过验证码来反爬
# - 反爬原理：对方服务器通过弹出验证码强制验证用户浏览行为
# - 解决方法：打码平台或者是机器学习的方法识别验证码，其中打码平台廉价易用，更值得推荐



# 二、基于爬虫行为进行反爬
# 1 基于请求频率或总请求数量
# > 爬虫的行为与普通用户有着明显的区别，爬虫的请求频率与请求次数要远高于普通用户

# 1.1 通过请求ip/账号单位时间内总请求数量进行反爬
# - 反爬原理：正常浏览器请求网站，速度不会太快，同一个ip/账号大量请求了对方服务器，有更大的可能性会被识别为爬虫
# - 解决方法：对应的通过购买高质量的ip的方式能够解决问题/购买个多账号

# 1.2 通过同一ip/账号请求之间的间隔进行反爬
# - 反爬原理：正常人操作浏览器浏览网站，请求之间的时间间隔是随机的，而爬虫前后两个请求之间时间间隔通常比较固定同时时间间隔较短，因此可以用来做反爬
# - 解决方法：请求之间进行随机等待，模拟真实用户操作，在添加时间间隔后，为了能够高速获取数据，尽量使用代理池，如果是账号，则将账号请求之间设置随机休眠

# 1.3 通过对请求ip/账号每天请求次数设置阈值进行反爬
# - 反爬原理：正常的浏览行为，其一天的请求次数是有限的，通常超过某一个值，服务器就会拒绝响应
# - 解决方法：对应的通过购买高质量的ip的方法/多账号，同时设置请求间随机休眠

# 2 根据爬取行为进行反爬，通常在爬取步骤上做分析

# 2.1 通过js实现跳转来反爬
# - 反爬原理：js实现页面跳转，无法在源码中获取下一页url
# - 解决方法: 多次抓包获取条状url，分析规律

# 2.2 通过蜜罐(陷阱)获取爬虫ip(或者代理ip)，进行反爬
# - 反爬原理：在爬虫获取链接进行请求的过程中，爬虫会根据正则，xpath，css等方式进行后续链接的提取，此时服务器端可以设置一个陷阱url，会被提取规则获取，但是正常用户无法获取，这样就能有效的区分爬虫和正常用户
# - 解决方法: 完成爬虫的编写之后，使用代理批量爬取测试/仔细分析响应内容结构，找出页面中存在的陷阱

# 2.3 通过假数据反爬
# - 反爬原理：向返回的响应中添加假数据污染数据库，通常家属剧不会被正常用户看到
# - 解决方法: 长期运行，核对数据库中数据同实际页面中数据对应情况，如果存在问题/仔细分析响应内容

# 2.4 阻塞任务队列
# - 反爬原理：通过生成大量垃圾url，从而阻塞任务队列，降低爬虫的实际工作效率
# - 解决方法: 观察运行过程中请求响应状态/仔细分析源码获取垃圾url生成规则，对URL进行过滤

# 2.5 阻塞网络IO
# - 反爬原理：发送请求获取响应的过程实际上就是下载的过程，在任务队列中混入一个大文件的url，当爬虫在进行该请求时将会占用网络io，如果是有多线程则会占用线程
# - 解决方法: 观察爬虫运行状态/多线程对请求线程计时/发送请求钱

# 2.6 运维平台综合审计
# - 反爬原理：通过运维平台进行综合管理，通常采用复合型反爬虫策略，多种手段同时使用
# - 解决方法: 仔细观察分析，长期运行测试目标网站，检查数据采集速度，多方面处理



# 三、基于数据加密进行反爬
# 1 对响应中含有的数据进行特殊化处理
# > 通常的特殊化处理主要指的就是css数据偏移/自定义字体/数据加密/数据图片/特殊编码格式等

# 1.1 通过自定义字体来反爬
# 例如：猫眼电影的口碑和票房数字 https://www.maoyan.com/films/1337941
# - 反爬思路: 使用自有字体文件
# - 解决思路：解析字体文件进行翻译 或者 切换到手机版尝试

# 1.2 通过css来反爬
# 例如：去哪儿网票价 https://flight.qunar.com/site/oneway_list_inter.htm?
# - 反爬思路：源码数据不为真正数据，需要通过css位移才能产生真正数据
# - 解决思路：计算css的偏移

# 1.3 通过js动态生成数据进行反爬
# - 反爬原理：通过js动态生成
# - 解决思路：解析关键js，获得数据生成流程，模拟生成数据

# 1.4 通过数据图片化反爬
# - 58同城短租](https://baise.58.com/duanzu/38018718834984x.shtml)
# - 解决思路：通过使用图片解析引擎从图片中解析数据

# 1.5 通过编码格式进行反爬
# - 反爬原理: 不适用默认编码格式，在获取响应之后通常爬虫使用utf-8格式进行解码，此时解码结果将会是乱码或者报错
# - 解决思路：根据源码进行多格式解码，或者真正的解码格式






# 图片验证码（CAPTCHA）
#  > “Completely Automated Public Turing test to tell Computers and Humans Apart”（全自动区分计算机和人类的图灵测试）的缩写，是一种区分用户是计算机还是人的公共全自动程序
# 作用：防止恶意破解密码、刷票、论坛灌水、刷页。有效防止某个黑客对某一个特定注册用户用特定程序暴力破解方式进行不断的登录尝试，实际上使用验证码是现在很多网站通行的方式（比如招商银行的网上个人银行，百度社区），我们利用比较简易的方式实现了这个功能。虽然登录麻烦一点，但是对网友的密码安全来说这个功能还是很有必要，也很重要。
# 在爬虫中的使用场景
# - 注册
# - 登录
# - 频繁发送请求时，服务器弹出验证码进行验证
# 处理方案
# - 手动输入(input)
#   这种方法仅限于登录一次就可持续使用的情况
# - 图像识别引擎解析
#   使用光学识别引擎处理图片中的数据，目前常用于图片数据提取，较少用于验证码处理
# - 打码平台
#   爬虫常用的验证码解决方案


# 图片识别引擎
# > OCR（Optical Character Recognition）是指使用扫描仪或数码相机对文本资料进行扫描成图像文件，然后对图像文件进行分析处理，自动识别获取文字信息及版面信息的软件
# 2.1 什么是tesseract
# - Tesseract，一款由HP实验室开发由Google维护的开源OCR引擎，特点是开源，免费，支持多语言，多平台。
# - 项目地址：https://github.com/tesseract-ocr/tesseract

# 2.2 图片识别引擎环境的安装
# 2.2.1 引擎的安装
# mac  brew install tesseract
# linux  sudo apt-get install tesseract-ocr
# windows  通过exe安装包安装，下载地址可以从GitHub项目中的wiki找到。安装完成后记得将Tesseract 执行文件的目录加入到PATH中，方便后续调用
# 2.2.2 Python库的安装
# PIL模块用于打开图片文件
# pip/pip3 install pillow
# pytesseract模块用于从图片中解析数据，调用引擎
# pip/pip3 install pytesseract

# 2.3 图片识别引擎的使用(处理英文文本效果较好)
# - 通过调用tesseract命令，解析图片, hDh /Users/hz/Desktop/hz/pic/ocr_test.png result.txt
# - 通过pytesseract模块的 image_to_string 方法就能将打开的图片文件中的数据提取成字符串数据，具体方法如下
import js2py
from PIL import Image
import pytesseract
im = Image.open('/Users/hz/Desktop/hz/pic/ocr_test.png', 'r')
result = pytesseract.image_to_string(image=im)
print(result)               # 中文解析有误
# tesseract简单使用训练：https://www.cnblogs.com/cnlian/p/5765871.html
# 其他ocr平台： 国内ocr对中文识别相对较好
#     微软Azure 图像识别：https://azure.microsoft.com/zh-cn/services/cognitive-services/computer-vision/
#     有道智云文字识别：http://aidemo.youdao.com/ocrdemo
#     阿里云图文识别：https://www.aliyun.com/product/cdi/
#     腾讯OCR文字识别：https://cloud.tencent.com/product/ocr


# 打码平台
# 1.现在很多网站都会使用验证码来进行反爬，所以为了能够更好的获取数据，需要了解如何使用打码平台爬虫中的验证码
# 2.常见的打码平台
# 2.1 云打码：http://www.yundama.com/
#    能够解决通用的验证码识别

# 2.2 极验验证码智能识别辅助：http://jiyandoc.c2567.com/
#    能够解决复杂验证码的识别

# 3.云打码的使用
# 下面以云打码为例，了解打码平台如何使用
# 3.1 云打码官方接口
# 下面代码是云打码平台提供，做了个简单修改，实现了两个方法：
# 3.1.1 indetify:传入图片的响应二进制数即可
# 3.1.2 indetify_by_filepath:传入图片的路径即可识别
# 其中需要自己配置的地方是：
# username = 'whoarewe' # 用户名
# password = '***' # 密码
# appid = 4283 # appid
# appkey = '02074c64f0d0bb9efb2df455537b01c3' # appkey
# codetype = 1004 # 验证码类型
# 云打码官方提供的api：
# ------->   ./_yundama.py

# 4.常见的验证码的种类
# 4.1 url地址不变，验证码不变
# 这是验证码里面非常简单的一种类型，对应的只需要获取验证码的地址，然后请求，通过打码平台识别即可
# 4.2 url地址不变，验证码变化
# 这种验证码的类型是更加常见的一种类型，对于这种验证码，大家需要思考：
# >在登录的过程中，假设我输入的验证码是对的，对方服务器是如何判断当前我输入的验证码是显示在我屏幕上的验证码，而不是其他的验证码呢？
# 在获取网页的时候，请求验证码，以及提交验证码的时候，对方服务器肯定通过了某种手段验证我之前获取的验证码和最后提交的验证码是同一个验证码，那这个手段是什么手段呢？
# 很明显，就是通过cookie来实现的，所以对应的，在请求页面，请求验证码，提交验证码的到时候需要保证cookie的一致性，对此可以使用requests.session来解决



# chrome浏览器无痕模式
# 使用隐身窗口，首次打开网站，不会带上cookie，能够观察页面的获取情况，包括对方服务器如何设置cookie在本地




# JS的解析    js2py的使用----纯python实现的js的解释器
# 1 确定js的位置
# 1.1 观察按钮的元素绑定js事件(箭头定位元素)
# 1.2 通过search all file 来搜索(右上角搜索)
# 1.3 查看请求的initiator(network)
# 2 观察js的执行过程
# 3 js2py的使用-------纯python实现的js的解释器
import js2py
import requests

# 创建js执行环境
context = js2py.EvalJs()

# 加载依赖的js文件
headers = {
    'User-Agent': ''
}
dowload_js_ = requests.get('', headers=headers).content.decode()
context.execute(dowload_js_)


context.execute('fun')    # 执行下载的js文件中的函数

# 给管理器添加变量
context.n = {'name': 'hhhhh'}
print(context.n)










































